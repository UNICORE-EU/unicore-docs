.. _ucc_tunneling:

|tunneling-img| Port forwarding / tunneling
-------------------------------------------

.. |tunneling-img| image:: ../../_static/tunneling.png
	:height: 32px
	:align: middle

Starting with UNICORE 9.1.0, it is possible to open a tunnel (TCP socket connection) from
the client to a service running on the HPC cluster. The service can run on a login node
or on a compute node.

Since this mechanism uses only the established UNICORE communication channels, it will work
in any situation, unhindered by firewalls.

Traffic is forwarded from the client through the UNICORE HTTPS stack *down* to the
cluster login node, where the (:ref:`UNICORE TSI <tsi>`) starts a process which connects
to the backend service and forwards data back through the UNICORE stack to the client.
So there is chain of connections, forwarding data through the following stack

 * Client application
 * Client-side listener (e.g. UCC)
 * Gateway
 * UNICORE/X
 * Server-side listener (TSI process) on the login node
 * Service
 
(in both directions).

That is quite a number of *hops*, so latency and throughput will be limited accordingly. As soon
as one of the connections is closed, the whole stack is shut down.

To establish the client side, UCC has a command ``open-tunnel``, which behaves similarly to an
SSH tunnel (``ssh -L ...``)

It is started by
 
.. code:: console

  $ ucc open-tunnel -L <listen-port> <endpoint>

The ``listen-port`` is the port where a local application can connect. You can use "0" to use any free port.

The ``endpoint`` is a UNICORE job endpoint URL, with a few extra parameters added:

``/forward-port?port=<service_port>&host=<service_host>&loginNode=<tsinode``

The ``port`` parameter is mandatory, and denotes the port where the backend service is listening.

The ``host`` and ``loginNode`` are optional:

 * ``host`` is the host where the service is running, must be reachable from the TSI (login node). 
   It defaults to ``localhost`` (as seen from the login node!).

 * ``loginNode`` is useful in cases where there are multiple login nodes, and you wish to control
   on which login node the forwarding process is launched.

Upon connection, the tunneling process is initiated, and the forwarding of data is started.
To stop listening and forwarding, press :kbd:`Control-C`, or use some other method to stop the UCC process.



Example
~~~~~~~

While usually the backend service is also started via UNICORE, that is not strictly necessary.
Any of your job endpoints will do.

In this example, however, we launch a Python web server via UNICORE, and then connect to
that Python service via a tunnel. In the first step, the service will run on a login node

Launch a :ref:`UCC shell <ucc_shell>` with ``ucc shell ...`` and run the following job to 
start the service, which will be listening on port 8877:

.. code:: console

  run -a

  { 
    "Executable": "python3 -m http.server 8877",
    "Job type":   "ON_LOGIN_NODE"
  }

  (type CTRL-D to to launch the job)

Make sure to wait until this job is running, i.e.

.. code:: console

  job-status $_
  
shows it as **RUNNING**. The UCC shell special variable ``$_`` automatically contains
the last URL, i.e. the new job's URL.

To open the tunnel:

.. code:: console

  open-tunnel -L 4321 $_/forward-port?port=8877
  

this will open a local listener on port 4321.


To test your tunnel, run something like the following (from ANOTHER terminal, **don't kill UCC**):

.. code:: console

  wget http://localhost:4321/stdout
  
You might also try and open \"http://localhost:4321\" in a browser.

Level 2: connecting to a service running on a compute node
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The next step is to connect to a service running on a compute node.
To be able to do that, you need to know which compute node it is, so UNICORE
can connect to it via the TSI.

There are several ways to achieve that. One way would be to check
the job's low-level details and get the node from there.

For example, tunning the following job:

.. code:: console

  run -a

  { 
    "Executable": "python3 -m http.server 8877"
  }

  (type CTRL-D to to launch the job)

Once it is running, check its details:

.. code:: console

  rest get $_/details

If your cluster is running SLURM, the host name(s) will be in the "NodeList" field.

With that, you can launch the tunnel via

.. code:: console

  open-tunnel -L 4321 $_/forward-port?port=8877&host=...

making sure to add the name/IP of the compute node via the "&host=..." parameter.

If this is not enoough (for example if your job is multi-node), the best way
is to write the hostname where your service is running to a file in the job
directory, and read that from the client.

Final notes
~~~~~~~~~~~
.. attention::
 **USE RESPONSIBLY!** This tool is not intended for high volume data streaming or a very high number of
 concurrent connections, since it does incur some overhead on the UNICORE infrastructure.
